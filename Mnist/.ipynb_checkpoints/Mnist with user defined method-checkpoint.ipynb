{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras.datasets.mnist' from 'C:\\\\Users\\\\Abirk\\\\anaconda3\\\\lib\\\\site-packages\\\\keras\\\\datasets\\\\mnist.py'>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_origin, Y_train_origin), (X_test_origin, Y_test_origin) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_origin.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 3\n",
      "x = [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  12  56 140 126 175 200  96   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  35 166 238 254 246 242 253 246 254  67\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 184 182 146 127  70  30  45  36 215 175\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30   0   0   0   0   0   0   0 207 246\n",
      "   14   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  55 251 169\n",
      "    1   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  11 215 232  20\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  20 190 250  61   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  24 118 206 254 248 142 108\n",
      "   18   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  63 223 254 254 254 254 254 254\n",
      "  209   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  52 174 129  95  16  16  16 106\n",
      "  249 125   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  179 239   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   80 239   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   80 244  20   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  100 239   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  234 239   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   4 140   5   0   0   0   0   0   0   3 150\n",
      "  254 129   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  64 254 181  38   0   0   0   0  34 188 254\n",
      "  209  20   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  12 226 255 223  88  68 128 157 242 254 207\n",
      "   23   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  45 210 254 254 254 254 255 254 187  49\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  41 129 239 229 179  91  16   3   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 50\n",
    "plt.imshow(X_train_origin[index])\n",
    "print (\"y = \" + str(Y_train_origin[index]))\n",
    "print (\"x = \" + str(X_train_origin[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples in x: m_train_x = 60000\n",
      "Number of training examples in y: m_train_y = 60000\n",
      "Number of test examples in x: m_test_x = 10000\n",
      "Number of test examples in x: m_test_y = 10000\n",
      "training input image shape in pixel = (28, 28)\n",
      "test input image shape in pixel = (28, 28)\n"
     ]
    }
   ],
   "source": [
    "## store all the shapes..\n",
    "m_train_x_size = X_train_origin.shape[0]\n",
    "m_train_y_size = Y_train_origin.shape[0]\n",
    "m_test_x_size = X_test_origin.shape[0]\n",
    "m_test_y_size = Y_test_origin.shape[0]\n",
    "\n",
    "print (\"Number of training examples in x: m_train_x = \" + str(m_train_x_size))\n",
    "print (\"Number of training examples in y: m_train_y = \" + str(m_train_y_size))\n",
    "print (\"Number of test examples in x: m_test_x = \" + str(m_test_x_size))\n",
    "print (\"Number of test examples in x: m_test_y = \" + str(m_test_y_size))\n",
    "print (\"training input image shape in pixel = \" + str(X_train_origin.shape[1:]))\n",
    "print (\"test input image shape in pixel = \" + str(X_test_origin.shape[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flaten the array and \n",
    "## reshape from (no of images, width, height, color channel) to (width * height * color channel, no of images)\n",
    "\n",
    "X_train_flatten = X_train_origin.reshape(X_train_origin.shape[0], -1).T\n",
    "X_test_flatten = X_test_origin.reshape(X_test_origin.shape[0], -1).T\n",
    "\n",
    "Y_train_flatten = Y_train_origin.reshape(Y_train_origin.shape[0], 1).T\n",
    "Y_test_flatten = Y_test_origin.reshape(Y_test_origin.shape[0], 1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(1, 60000)\n",
      "(784, 60000)\n",
      "(1, 10000)\n",
      "\n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# keep all training pixel values between 0 and 1\n",
    "X_train = X_train_flatten/255\n",
    "X_test = X_test_flatten/255\n",
    "\n",
    "Y_train = Y_train_flatten\n",
    "Y_test = Y_test_flatten\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters..\n",
    "def initialize_parameters(size_of_input_layer, size_of_hidden_layer_1, size_of_output_layer):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(size_of_hidden_layer_1, size_of_input_layer) * 0.01\n",
    "    \n",
    "    b1 = np.zeros((size_of_hidden_layer_1, 1), dtype= float)\n",
    "    \n",
    "    W2 = np.random.randn(size_of_output_layer, size_of_hidden_layer_1) * 0.01\n",
    "    \n",
    "    b2 = np.zeros((size_of_output_layer, 1), dtype= float)\n",
    "    \n",
    "\n",
    "    assert(W1.shape == (size_of_hidden_layer_1, size_of_input_layer))\n",
    "    \n",
    "    assert(b1.shape == (size_of_hidden_layer_1, 1))\n",
    "    \n",
    "    assert(W2.shape == (size_of_output_layer, size_of_hidden_layer_1))\n",
    "           \n",
    "    assert(b2.shape == (size_of_output_layer, 1))\n",
    "\n",
    "    parameters = {\n",
    "        \"W1\":W1,\n",
    "        \"b1\":b1,\n",
    "        \"W2\":W2,\n",
    "        \"b2\":b2\n",
    "    }\n",
    "           \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward prop section start..\n",
    "\n",
    "$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. \n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def Sigmoid(Z):\n",
    "    res = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    cache = Z\n",
    "    return res, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def Softmax(Z):\n",
    "    res = np.exp(Z - Z.max())\n",
    "    res = res / np.sum(res, axis=0)\n",
    "    \n",
    "    cache = Z\n",
    "    return res, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Activation Functions and results..\n",
    "def Forward_Activation(A_prev, W, b, activation_function_to_apply):\n",
    "    \n",
    "    Z, linear_cache = Linear_Forward_Pass_Calculate_Z(A_prev, W, b)\n",
    "    \n",
    "    if activation_function_to_apply == 'sigmoid':\n",
    "        A, activation_cache = Sigmoid(Z)          # activation_cache contains Z. It will be used in back propagation..\n",
    "    elif activation_function_to_apply == 'softmax':\n",
    "        A, activation_cache = Softmax(Z)\n",
    "    \n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to get Z where Z = WT*A + b ....\n",
    "def Linear_Forward_Pass_Calculate_Z(A_prev, W, b): \n",
    "        # A = activation of previous layer\n",
    "        # W = Weights of current layer\n",
    "        # b = bias of current layer\n",
    "\n",
    "    Z = np.dot(W,A_prev) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    cache = (A_prev, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "def Compute_cost(Y_hat, Y): #Y_hat = prediction, Y = actual Output..\n",
    "    \n",
    "    m = Y.shape[1] # m = number_of_example_data\n",
    "    \n",
    "    cost = (-1/m)*np.sum(Y*np.log(Y_hat) + (1-Y)*np.log(1-Y_hat))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "                                 # np.squeeze()  = Remove axes of length one from a.\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop section start..\n",
    "\n",
    " compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    " $$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward sigmoid..\n",
    "def Backward_Sigmoid(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward softmax..\n",
    "def Backward_Softmax(dA_Final, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    res = np.exp(Z - Z.max())\n",
    "    res = res / np.sum(res, axis=0) * (1 - res / np.sum(res, axis=0))\n",
    "    \n",
    "    dZ = dA_Final * res\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Activation.. \n",
    "def Backward_activation(dA, linear_and_activation_cache, backwad_activation_to_apply):\n",
    "    \n",
    "    linear_cache, activation_cache = linear_and_activation_cache\n",
    "    \n",
    "    if backwad_activation_to_apply == \"sigmoid\":\n",
    "        \n",
    "        dZ = Backward_Sigmoid(dA, activation_cache)\n",
    "        \n",
    "        dA_prev, dW, db = Linear_Backward_Pass_Calculate_dW_A_db(dZ, linear_cache)\n",
    "        \n",
    "    elif backwad_activation_to_apply == \"softmax\":\n",
    "        \n",
    "        dZ = Backward_Softmax(dA, activation_cache)\n",
    "        dA_prev, dW, db = Linear_Backward_Pass_Calculate_dW_A_db(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear backward pass..\n",
    "def Linear_Backward_Pass_Calculate_dW_A_db(dZ, linear_cache):\n",
    "    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m)*np.dot(dZ,(A_prev.T))\n",
    "    db = (1/m)*np.sum(dZ, axis = 1, keepdims=True)\n",
    "    dA_prev = np.dot((W.T),dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update parameters\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate): #parameters are, W1, b1, W2, b2 ..[weights and biases]\n",
    "    \n",
    "    L = len(parameters) // 2   # number of layers in the neural network\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*gradients[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*gradients[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model\n",
    "\n",
    "Overall steps:\n",
    "    1. Initialize parameters / Define hyperparameters\n",
    "    2. Loop for num_iterations:\n",
    "        a. Forward propagation\n",
    "        b. Compute cost function\n",
    "        c. Backward propagation\n",
    "        d. Update parameters (using parameters, and grads from backprop) \n",
    "    4. Use trained parameters to predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Two_Layer_Neural_Model(X, Y, layers_dimensions, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    gradients = {}\n",
    "    costs = []\n",
    "    \n",
    "    m = X.shape[1]  # no of examples\n",
    "    \n",
    "    (size_of_input_layer, size_of_hidden_layer1, size_of_output_layer) = layers_dimensions \n",
    "    \n",
    "    #initialize\n",
    "    parameters = initialize_parameters(size_of_input_layer, size_of_hidden_layer1, size_of_output_layer)\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        #forward prop\n",
    "        A1, cache1 = Forward_Activation(X, W1, b1, activation_function_to_apply = \"sigmoid\")\n",
    "        A2, cache2 = Forward_Activation(A1, W2, b2, activation_function_to_apply = \"softmax\")\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = Compute_cost(A2, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = 2 * (A2 - Y) / A2.shape[0]\n",
    "        \n",
    "        #calculate gradients..\n",
    "        dA1, dW2, db2 = Backward_activation(dA2, cache2, backwad_activation_to_apply = \"softmax\")\n",
    "        dA0, dW1, db1 = Backward_activation(dA1, cache1, backwad_activation_to_apply = \"sigmoid\")\n",
    "        \n",
    "        #store gradients..\n",
    "        gradients['dW1'] = dW1\n",
    "        gradients['db1'] = db1\n",
    "        gradients['dW2'] = dW2\n",
    "        gradients['db2'] = db2\n",
    "        \n",
    "        #pdate parameters..\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 784     # num_px * num_px\n",
    "n_h = 128\n",
    "n_y = 10\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 311.83166035612226\n",
      "Cost after iteration 100: 314.9257379515943\n",
      "Cost after iteration 200: 500.29882806262486\n",
      "Cost after iteration 300: 821.1687573906438\n",
      "Cost after iteration 400: 947.580217437449\n",
      "Cost after iteration 500: 1006.6954627674347\n",
      "Cost after iteration 600: 1034.3698011779184\n"
     ]
    }
   ],
   "source": [
    "parameters = Two_Layer_Neural_Model(X_train, Y_train, layers_dimensions = (n_x, n_h, n_y), num_iterations = 200, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
